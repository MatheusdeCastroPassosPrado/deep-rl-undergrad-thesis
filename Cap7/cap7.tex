\section{Conclusions}
In this work, we presented several techniques for mimicking humanoid kick motion and, finally, presented a framework that is able to mimic a reference motion and optimize it towards a task.

For mimicking motions, we presented pure RL techniques -- starting from randomly policies until well defined kick movements -- that are agnostic to the task itself and hence could be applied in other motions, such as walk. However, these techniques has some problems:

\begin{itemize}
	\item If we do not expose a reference motion, agent will learn a sub-optimal policy which is very different from human behavior;
	\item If we just pass a reference motion, agent prefers to stand still because exploration leads to higher penalizations; and
	\item Even if we have a reference motion and several techniques for reducing sampling bad data, the agent was not able to conduct the mimicked motion properly -- the residual error collapsed the motion.
\end{itemize}



We also presented a Hybrid Learning Model, which initially trains a neural network policy using supervised learning from the reference motion and optimizes it towards a task using Deep RL techniques. \textbf{It proved to work well and achieved the objective of improve policy performance, which was, in this case, to gain accuracy}. The supervised learning step provided high initial reward and a good point to start exploration and the RL algorithm could absorb such benefits and find a better policy than the original motion.

Therefore, we conclude that pure RL techniques have a greater difficulty due to the fact that, starting from an initially randomized parametrized policy, we have a much more complicated optimization problem, where it should be considered
several aspects for modeling the reward to at least mimic the reference motion. On the other hand, when using a supervised pre training, it is possible to obtain an initial policy that replicates well the reference motion and is able to be optimized using RL and, finally, outperforms this reference when we consider a performance task.

\section{Future Work}

We propose several research lines from this work.

First of all, we consider the replication of this methodology in other reference motions or even in different types of motion. As described in previous section, we've used a reference kick without much stability - which compromises the pure RL techniques presented here. With more stable kicks, not just pure RL algorithms could be able to learn motions perfectly, but it could also allow greater exploration and learning rate hyperparameters in optimization steps, even when using Hybrid Learning Models. It could start from a simpler motion but achieves better policies than those from our experimentations.

A second line of research is to apply these methods in harder motions than keyframed ones. For example, apply the methods described here in order to learn a closed-loop walking engine based on model-free Reinforcement Learning.

A third idea is to try policy optimization by reference motion improvement. It is similar to Reinforcement Learning by self-play used in games: we start from a reference motion and replaces it from other motion that has better reward. This would lead to always improve the reference motion towards to optimize the task execution.

Other interesting idea to follow is to theoretically derive the relation between supervised leaning and the usage of RL algorithms for supervised tasks as used in section \ref{sec:suprl} and analyze the convergence in this case.

In terms of distributed training, we aim to better analyze the situations of parallelism and how the number of agents affect a specific training. Furthermore, we think in exploring better the available hardware from Intel DevCloud.


Other future line of work proposed here is related to Policy Gradients algorithms. It is known that these kind of algorithms has poor data efficiency and are very sensible to some hyperparameters. Hence, we propose the study of data sampling techniques in the context of these algorithms and the study of Meta-Learning techniques \cite{metalearning} to adaptively improve the hyperparameters.


