\select@language {english}
\contentsline {chapter}{\numberline {1}Introduction}{19}{chapter.1}
\contentsline {section}{\numberline {1.1}Motivation}{19}{section.1.1}
\contentsline {section}{\numberline {1.2}Contextualization}{19}{section.1.2}
\contentsline {section}{\numberline {1.3}Objective}{21}{section.1.3}
\contentsline {section}{\numberline {1.4}Scope}{21}{section.1.4}
\contentsline {section}{\numberline {1.5}Organization of this work}{21}{section.1.5}
\contentsline {chapter}{\numberline {2}Literature Review}{23}{chapter.2}
\contentsline {section}{\numberline {2.1}The RoboCup Soccer3D Simulation League}{23}{section.2.1}
\contentsline {subsection}{\numberline {2.1.1}Domain Description}{23}{subsection.2.1.1}
\contentsline {subsection}{\numberline {2.1.2} Kick Motion }{23}{subsection.2.1.2}
\contentsline {subsection}{\numberline {2.1.3}Keyframe Movements}{24}{subsection.2.1.3}
\contentsline {subsection}{\numberline {2.1.4}Optimization Techniques}{25}{subsection.2.1.4}
\contentsline {section}{\numberline {2.2}Reinforcement Learning for Control}{26}{section.2.2}
\contentsline {chapter}{\numberline {3}Deep Learning Background}{29}{chapter.3}
\contentsline {section}{\numberline {3.1}Neural Networks}{29}{section.3.1}
\contentsline {subsection}{\numberline {3.1.1}A Neuron}{30}{subsection.3.1.1}
\contentsline {subsection}{\numberline {3.1.2}Neural Network Representation}{31}{subsection.3.1.2}
\contentsline {subsection}{\numberline {3.1.3}Vectorization}{32}{subsection.3.1.3}
\contentsline {section}{\numberline {3.2}Activation Functions}{33}{section.3.2}
\contentsline {subsection}{\numberline {3.2.1}Logistic Sigmoid}{33}{subsection.3.2.1}
\contentsline {subsection}{\numberline {3.2.2}Hyperbolic Tangent}{33}{subsection.3.2.2}
\contentsline {subsection}{\numberline {3.2.3}Rectified Linear Unit - ReLU}{35}{subsection.3.2.3}
\contentsline {subsection}{\numberline {3.2.4}Leaky ReLU}{36}{subsection.3.2.4}
\contentsline {section}{\numberline {3.3}Cost Function}{36}{section.3.3}
\contentsline {section}{\numberline {3.4}Gradient Descent}{37}{section.3.4}
\contentsline {section}{\numberline {3.5}Backpropagation}{39}{section.3.5}
\contentsline {section}{\numberline {3.6}Optimization Algorithms}{39}{section.3.6}
\contentsline {subsection}{\numberline {3.6.1}Batch, Mini-batch and Stochastic Gradient Descent}{40}{subsection.3.6.1}
\contentsline {subsection}{\numberline {3.6.2}Momentum}{42}{subsection.3.6.2}
\contentsline {subsection}{\numberline {3.6.3}RMSProp}{44}{subsection.3.6.3}
\contentsline {subsection}{\numberline {3.6.4}Adam}{45}{subsection.3.6.4}
\contentsline {section}{\numberline {3.7}Weights Random Initialization}{45}{section.3.7}
\contentsline {subsection}{\numberline {3.7.1}Xavier Initialization}{47}{subsection.3.7.1}
\contentsline {section}{\numberline {3.8}Gradient Descent convergence and learning rate decay}{47}{section.3.8}
\contentsline {chapter}{\numberline {4}Reinforcement Learning Background}{50}{chapter.4}
\contentsline {section}{\numberline {4.1}Concepts of a Reinforcement Learning System}{50}{section.4.1}
\contentsline {section}{\numberline {4.2}Reinforcement Learning System}{51}{section.4.2}
\contentsline {subsection}{\numberline {4.2.1}Reward}{51}{subsection.4.2.1}
\contentsline {subsection}{\numberline {4.2.2}State}{52}{subsection.4.2.2}
\contentsline {subsection}{\numberline {4.2.3}Policy}{53}{subsection.4.2.3}
\contentsline {subsection}{\numberline {4.2.4}Value Function}{53}{subsection.4.2.4}
\contentsline {subsection}{\numberline {4.2.5}Model}{54}{subsection.4.2.5}
\contentsline {section}{\numberline {4.3}Markov Decision Process}{54}{section.4.3}
\contentsline {subsection}{\numberline {4.3.1}Markov State}{54}{subsection.4.3.1}
\contentsline {subsection}{\numberline {4.3.2}State Transition Matrix}{55}{subsection.4.3.2}
\contentsline {subsection}{\numberline {4.3.3}Markov Decision Process}{55}{subsection.4.3.3}
\contentsline {subsection}{\numberline {4.3.4}State-Value and Action-Value Function}{57}{subsection.4.3.4}
\contentsline {section}{\numberline {4.4}Bellman Equation}{58}{section.4.4}
\contentsline {subsection}{\numberline {4.4.1}Bellman Expectation Equation}{58}{subsection.4.4.1}
\contentsline {subsection}{\numberline {4.4.2}Bellman Optimality Equation}{59}{subsection.4.4.2}
\contentsline {section}{\numberline {4.5}Exact Solution Methods}{61}{section.4.5}
\contentsline {subsection}{\numberline {4.5.1}Policy Iteration}{61}{subsection.4.5.1}
\contentsline {subsubsection}{\numberline {4.5.1.1}Policy Evaluation}{62}{subsubsection.4.5.1.1}
\contentsline {subsubsection}{\numberline {4.5.1.2}Policy Improvement}{62}{subsubsection.4.5.1.2}
\contentsline {subsubsection}{\numberline {4.5.1.3}Policy Iteration algorithm}{63}{subsubsection.4.5.1.3}
\contentsline {subsection}{\numberline {4.5.2}Value Iteration}{63}{subsection.4.5.2}
\contentsline {subsection}{\numberline {4.5.3}Limitations of Exact Solution Methods}{64}{subsection.4.5.3}
\contentsline {section}{\numberline {4.6}Policy Gradient Methods}{65}{section.4.6}
\contentsline {subsection}{\numberline {4.6.1}Policy Gradient Theorem}{66}{subsection.4.6.1}
\contentsline {subsection}{\numberline {4.6.2}Actor-Critic Models}{67}{subsection.4.6.2}
\contentsline {subsection}{\numberline {4.6.3}Advantage Function and GAE Algorithm}{69}{subsection.4.6.3}
\contentsline {section}{\numberline {4.7}Advanced Policy Gradient Methods}{70}{section.4.7}
\contentsline {subsection}{\numberline {4.7.1}Optimization Loss}{70}{subsection.4.7.1}
\contentsline {subsection}{\numberline {4.7.2}Trust Region Policy Optimization -- TRPO}{71}{subsection.4.7.2}
\contentsline {subsection}{\numberline {4.7.3}Proximal Policy Optimization -- PPO}{72}{subsection.4.7.3}
\contentsline {subsubsection}{\numberline {4.7.3.1}Clipped Surrogate Loss}{72}{subsubsection.4.7.3.1}
\contentsline {subsubsection}{\numberline {4.7.3.2}PPO Algorithm}{73}{subsubsection.4.7.3.2}
\contentsline {chapter}{\numberline {5}Methodology}{75}{chapter.5}
\contentsline {section}{\numberline {5.1}The Kick Motion Problem}{75}{section.5.1}
\contentsline {section}{\numberline {5.2}Experimentation Setup}{75}{section.5.2}
\contentsline {subsection}{\numberline {5.2.1}Reinforcement with Naive Reward - RNR }{76}{subsection.5.2.1}
\contentsline {subsection}{\numberline {5.2.2}Reinforcement with Reference Reward - RRR }{76}{subsection.5.2.2}
\contentsline {subsection}{\numberline {5.2.3}Reinforcement with Initial State Distribution - RISD}{76}{subsection.5.2.3}
\contentsline {subsection}{\numberline {5.2.4}Reinforcement with Early Termination - RET}{77}{subsection.5.2.4}
\contentsline {subsection}{\numberline {5.2.5}Hybrid Learning Model -- HLM}{77}{subsection.5.2.5}
\contentsline {section}{\numberline {5.3}Supervised Learning Setup}{78}{section.5.3}
\contentsline {subsection}{\numberline {5.3.1}The Dataset}{78}{subsection.5.3.1}
\contentsline {subsection}{\numberline {5.3.2}Neural Network Architecture and Hyperparameters}{78}{subsection.5.3.2}
\contentsline {subsection}{\numberline {5.3.3}The Training Procedure}{79}{subsection.5.3.3}
\contentsline {subsection}{\numberline {5.3.4}The Deployment in the Soccer 3D Environment}{80}{subsection.5.3.4}
\contentsline {section}{\numberline {5.4}Reinforcement Learning Setup}{80}{section.5.4}
\contentsline {subsection}{\numberline {5.4.1}Policy Representation}{80}{subsection.5.4.1}
\contentsline {subsubsection}{\numberline {5.4.1.1}Input Normalization Filter}{80}{subsubsection.5.4.1.1}
\contentsline {subsubsection}{\numberline {5.4.1.2}Neural Network}{81}{subsubsection.5.4.1.2}
\contentsline {subsubsection}{\numberline {5.4.1.3}Gaussian Action Space Noise}{82}{subsubsection.5.4.1.3}
\contentsline {subsection}{\numberline {5.4.2}Task Description}{82}{subsection.5.4.2}
\contentsline {section}{\numberline {5.5}Infrastructure}{83}{section.5.5}
\contentsline {subsection}{\numberline {5.5.1}Reinforcement Learning Server}{83}{subsection.5.5.1}
\contentsline {subsubsection}{\numberline {5.5.1.1}Simulation Server}{83}{subsubsection.5.5.1.1}
\contentsline {subsubsection}{\numberline {5.5.1.2}Soccer Agent}{84}{subsubsection.5.5.1.2}
\contentsline {subsubsection}{\numberline {5.5.1.3}Learning Agent}{84}{subsubsection.5.5.1.3}
\contentsline {subsection}{\numberline {5.5.2}Neural Network Deployment}{85}{subsection.5.5.2}
\contentsline {subsubsection}{\numberline {5.5.2.1}Import Supervised Model into OpenAI Baselines}{85}{subsubsection.5.5.2.1}
\contentsline {subsubsection}{\numberline {5.5.2.2}Export OpenAI Baselines onto Soccer Agent}{86}{subsubsection.5.5.2.2}
\contentsline {subsection}{\numberline {5.5.3}Distributed Training}{88}{subsection.5.5.3}
\contentsline {subsubsection}{\numberline {5.5.3.1}Data Parallelism}{88}{subsubsection.5.5.3.1}
\contentsline {subsubsection}{\numberline {5.5.3.2}Synchronous vs. Asynchronous Distributed Training}{89}{subsubsection.5.5.3.2}
\contentsline {subsection}{\numberline {5.5.4}Metrics}{90}{subsection.5.5.4}
\contentsline {subsection}{\numberline {5.5.5}Monitoring via Tensorboard}{91}{subsection.5.5.5}
\contentsline {chapter}{\numberline {6}Results' Analysis and Discussion}{92}{chapter.6}
\contentsline {section}{\numberline {6.1}Distributed Training}{92}{section.6.1}
\contentsline {section}{\numberline {6.2}Pure Reinforcement Learning methods results}{93}{section.6.2}
\contentsline {subsection}{\numberline {6.2.1}RNR}{93}{subsection.6.2.1}
\contentsline {subsection}{\numberline {6.2.2}RRR}{95}{subsection.6.2.2}
\contentsline {subsection}{\numberline {6.2.3}RNR+RRR}{95}{subsection.6.2.3}
\contentsline {subsection}{\numberline {6.2.4}RNR+RRR+RISD}{97}{subsection.6.2.4}
\contentsline {subsection}{\numberline {6.2.5}RNR+RRR+RISD+RET}{99}{subsection.6.2.5}
\contentsline {subsection}{\numberline {6.2.6}Reinforcement ``supervised"}{100}{subsection.6.2.6}
\contentsline {subsection}{\numberline {6.2.7}Other ideas for pure RL techniques}{101}{subsection.6.2.7}
\contentsline {section}{\numberline {6.3}Hybrid Learning Models}{102}{section.6.3}
\contentsline {subsection}{\numberline {6.3.1}Supervised Learning Results}{102}{subsection.6.3.1}
\contentsline {subsubsection}{\numberline {6.3.1.1}Training Results}{102}{subsubsection.6.3.1.1}
\contentsline {subsubsection}{\numberline {6.3.1.2}The Learned Kick Motion}{103}{subsubsection.6.3.1.2}
\contentsline {subsubsection}{\numberline {6.3.1.3}The Learned Walk Motion}{105}{subsubsection.6.3.1.3}
\contentsline {subsubsection}{\numberline {6.3.1.4}Other motions}{106}{subsubsection.6.3.1.4}
\contentsline {subsection}{\numberline {6.3.2}HLM+RNR}{107}{subsection.6.3.2}
\contentsline {subsection}{\numberline {6.3.3}Final Model: HLM+RNR+RET}{107}{subsection.6.3.3}
\contentsline {subsection}{\numberline {6.3.4}Kick Behavior: Numerical Results}{110}{subsection.6.3.4}
\contentsline {chapter}{\numberline {7}Conclusions, Recommendations, and Future Works}{112}{chapter.7}
\contentsline {section}{\numberline {7.1}Conclusions}{112}{section.7.1}
\contentsline {section}{\numberline {7.2}Future Work}{113}{section.7.2}
\contentsline {chapter}{Bibliography}{114}{chapter.8}
\SetAppendixTocName {Appendix}
\contentsline {chapter}{\numberline {A}Hyperparameters}{121}{appendix.A}
