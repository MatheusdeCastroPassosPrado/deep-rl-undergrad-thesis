\select@language {english}
\contentsline {chapter}{\numberline {1}Introduction}{23}{chapter.1}
\contentsline {section}{\numberline {1.1}Motivation}{23}{section.1.1}
\contentsline {section}{\numberline {1.2}Contextualization}{23}{section.1.2}
\contentsline {section}{\numberline {1.3}Objective}{25}{section.1.3}
\contentsline {section}{\numberline {1.4}Scope}{25}{section.1.4}
\contentsline {section}{\numberline {1.5}Organization of this work}{26}{section.1.5}
\contentsline {chapter}{\numberline {2}Background and Literature Review}{27}{chapter.2}
\contentsline {section}{\numberline {2.1}The RoboCup Soccer 3D Simulation League}{27}{section.2.1}
\contentsline {subsection}{\numberline {2.1.1}Domain Description}{27}{subsection.2.1.1}
\contentsline {subsection}{\numberline {2.1.2} Kick Motion }{28}{subsection.2.1.2}
\contentsline {subsection}{\numberline {2.1.3}Keyframe Movements}{28}{subsection.2.1.3}
\contentsline {subsection}{\numberline {2.1.4}Optimization Techniques}{29}{subsection.2.1.4}
\contentsline {section}{\numberline {2.2}Reinforcement Learning for Control}{30}{section.2.2}
\contentsline {chapter}{\numberline {3}Deep Learning}{34}{chapter.3}
\contentsline {section}{\numberline {3.1}Neural Networks}{34}{section.3.1}
\contentsline {subsection}{\numberline {3.1.1}A Neuron}{35}{subsection.3.1.1}
\contentsline {subsection}{\numberline {3.1.2}Neural Network Representation}{36}{subsection.3.1.2}
\contentsline {subsection}{\numberline {3.1.3}Vectorization}{38}{subsection.3.1.3}
\contentsline {section}{\numberline {3.2}Activation Functions}{38}{section.3.2}
\contentsline {subsection}{\numberline {3.2.1}Logistic Sigmoid}{38}{subsection.3.2.1}
\contentsline {subsection}{\numberline {3.2.2}Hyperbolic Tangent}{39}{subsection.3.2.2}
\contentsline {subsection}{\numberline {3.2.3}Rectified Linear Unit - ReLU}{39}{subsection.3.2.3}
\contentsline {subsection}{\numberline {3.2.4}Leaky ReLU}{40}{subsection.3.2.4}
\contentsline {section}{\numberline {3.3}Cost Function}{41}{section.3.3}
\contentsline {section}{\numberline {3.4}Gradient Descent}{43}{section.3.4}
\contentsline {section}{\numberline {3.5}Backpropagation}{44}{section.3.5}
\contentsline {section}{\numberline {3.6}Optimization Algorithms}{45}{section.3.6}
\contentsline {subsection}{\numberline {3.6.1}Batch, Mini-batch and Stochastic Gradient Descent}{45}{subsection.3.6.1}
\contentsline {subsection}{\numberline {3.6.2}Momentum}{47}{subsection.3.6.2}
\contentsline {subsection}{\numberline {3.6.3}RMSProp}{49}{subsection.3.6.3}
\contentsline {subsection}{\numberline {3.6.4}Adam}{50}{subsection.3.6.4}
\contentsline {section}{\numberline {3.7}Weights Random Initialization}{50}{section.3.7}
\contentsline {subsection}{\numberline {3.7.1}Xavier Initialization}{52}{subsection.3.7.1}
\contentsline {section}{\numberline {3.8}Gradient Descent convergence and learning rate decay}{52}{section.3.8}
\contentsline {chapter}{\numberline {4}Reinforcement Learning}{55}{chapter.4}
\contentsline {section}{\numberline {4.1}Concepts of a Reinforcement Learning System}{55}{section.4.1}
\contentsline {section}{\numberline {4.2}Reinforcement Learning System}{56}{section.4.2}
\contentsline {subsection}{\numberline {4.2.1}Reward}{57}{subsection.4.2.1}
\contentsline {subsection}{\numberline {4.2.2}State}{57}{subsection.4.2.2}
\contentsline {subsection}{\numberline {4.2.3}Policy}{58}{subsection.4.2.3}
\contentsline {subsection}{\numberline {4.2.4}Value Function}{58}{subsection.4.2.4}
\contentsline {subsection}{\numberline {4.2.5}Model}{59}{subsection.4.2.5}
\contentsline {section}{\numberline {4.3}Markov Decision Process}{59}{section.4.3}
\contentsline {subsection}{\numberline {4.3.1}Markov State}{60}{subsection.4.3.1}
\contentsline {subsection}{\numberline {4.3.2}State Transition Matrix}{60}{subsection.4.3.2}
\contentsline {subsection}{\numberline {4.3.3}Markov Decision Process}{60}{subsection.4.3.3}
\contentsline {subsection}{\numberline {4.3.4}State-Value and Action-Value Function}{62}{subsection.4.3.4}
\contentsline {section}{\numberline {4.4}Bellman Equation}{63}{section.4.4}
\contentsline {subsection}{\numberline {4.4.1}Bellman Expectation Equation}{63}{subsection.4.4.1}
\contentsline {subsection}{\numberline {4.4.2}Bellman Optimality Equation}{64}{subsection.4.4.2}
\contentsline {section}{\numberline {4.5}Exact Solution Methods}{66}{section.4.5}
\contentsline {subsection}{\numberline {4.5.1}Policy Iteration}{66}{subsection.4.5.1}
\contentsline {subsubsection}{\numberline {4.5.1.1}Policy Evaluation}{67}{subsubsection.4.5.1.1}
\contentsline {subsubsection}{\numberline {4.5.1.2}Policy Improvement}{68}{subsubsection.4.5.1.2}
\contentsline {subsubsection}{\numberline {4.5.1.3}Policy Iteration algorithm}{68}{subsubsection.4.5.1.3}
\contentsline {subsection}{\numberline {4.5.2}Value Iteration}{69}{subsection.4.5.2}
\contentsline {subsection}{\numberline {4.5.3}Limitations of Exact Solution Methods}{69}{subsection.4.5.3}
\contentsline {section}{\numberline {4.6}Policy Gradient Methods}{70}{section.4.6}
\contentsline {subsection}{\numberline {4.6.1}Policy Gradient Theorem}{71}{subsection.4.6.1}
\contentsline {subsection}{\numberline {4.6.2}Actor-Critic Models}{73}{subsection.4.6.2}
\contentsline {subsection}{\numberline {4.6.3}Advantage Function and GAE Algorithm}{73}{subsection.4.6.3}
\contentsline {section}{\numberline {4.7}Advanced Policy Gradient Methods}{75}{section.4.7}
\contentsline {subsection}{\numberline {4.7.1}Optimization Loss}{76}{subsection.4.7.1}
\contentsline {subsection}{\numberline {4.7.2}Trust Region Policy Optimization -- TRPO}{76}{subsection.4.7.2}
\contentsline {subsection}{\numberline {4.7.3}Proximal Policy Optimization -- PPO}{77}{subsection.4.7.3}
\contentsline {subsubsection}{\numberline {4.7.3.1}Clipped Surrogate Loss}{77}{subsubsection.4.7.3.1}
\contentsline {subsubsection}{\numberline {4.7.3.2}PPO Algorithm}{78}{subsubsection.4.7.3.2}
\contentsline {chapter}{\numberline {5}Methodology}{80}{chapter.5}
\contentsline {section}{\numberline {5.1}The Kick Motion Problem}{80}{section.5.1}
\contentsline {section}{\numberline {5.2}Experimentation Setup}{81}{section.5.2}
\contentsline {subsection}{\numberline {5.2.1}Reinforcement with Naive Reward - RNR }{81}{subsection.5.2.1}
\contentsline {subsection}{\numberline {5.2.2}Reinforcement with Reference Reward - RRR }{81}{subsection.5.2.2}
\contentsline {subsection}{\numberline {5.2.3}Reinforcement with Initial State Distribution - RISD}{82}{subsection.5.2.3}
\contentsline {subsection}{\numberline {5.2.4}Reinforcement with Early Termination - RET}{82}{subsection.5.2.4}
\contentsline {subsection}{\numberline {5.2.5}Hybrid Learning Model -- HLM}{82}{subsection.5.2.5}
\contentsline {section}{\numberline {5.3}Supervised Learning Setup}{83}{section.5.3}
\contentsline {subsection}{\numberline {5.3.1}The Dataset}{83}{subsection.5.3.1}
\contentsline {subsection}{\numberline {5.3.2}Neural Network Architecture and Hyperparameters}{84}{subsection.5.3.2}
\contentsline {subsection}{\numberline {5.3.3}The Training Procedure}{85}{subsection.5.3.3}
\contentsline {subsection}{\numberline {5.3.4}The Deployment in the Soccer 3D Environment}{85}{subsection.5.3.4}
\contentsline {section}{\numberline {5.4}Reinforcement Learning Setup}{85}{section.5.4}
\contentsline {subsection}{\numberline {5.4.1}Policy Representation}{86}{subsection.5.4.1}
\contentsline {subsubsection}{\numberline {5.4.1.1}Input Normalization Filter}{86}{subsubsection.5.4.1.1}
\contentsline {subsubsection}{\numberline {5.4.1.2}Neural Network}{86}{subsubsection.5.4.1.2}
\contentsline {subsubsection}{\numberline {5.4.1.3}Gaussian Action Space Noise}{87}{subsubsection.5.4.1.3}
\contentsline {subsection}{\numberline {5.4.2}Task Description}{87}{subsection.5.4.2}
\contentsline {section}{\numberline {5.5}Infrastructure}{89}{section.5.5}
\contentsline {subsection}{\numberline {5.5.1}Reinforcement Learning Server}{89}{subsection.5.5.1}
\contentsline {subsubsection}{\numberline {5.5.1.1}Simulation Server}{89}{subsubsection.5.5.1.1}
\contentsline {subsubsection}{\numberline {5.5.1.2}Soccer Agent}{89}{subsubsection.5.5.1.2}
\contentsline {subsubsection}{\numberline {5.5.1.3}Learning Agent}{90}{subsubsection.5.5.1.3}
\contentsline {subsection}{\numberline {5.5.2}Neural Network Deployment}{90}{subsection.5.5.2}
\contentsline {subsubsection}{\numberline {5.5.2.1}Importing Supervised Model into OpenAI Baselines}{90}{subsubsection.5.5.2.1}
\contentsline {subsubsection}{\numberline {5.5.2.2}Exporting OpenAI Baselines onto Soccer Agent}{91}{subsubsection.5.5.2.2}
\contentsline {subsection}{\numberline {5.5.3}Distributed Training}{92}{subsection.5.5.3}
\contentsline {subsubsection}{\numberline {5.5.3.1}Data Parallelism}{93}{subsubsection.5.5.3.1}
\contentsline {subsubsection}{\numberline {5.5.3.2}Synchronous vs. Asynchronous Distributed Training}{94}{subsubsection.5.5.3.2}
\contentsline {subsection}{\numberline {5.5.4}Metrics}{95}{subsection.5.5.4}
\contentsline {subsection}{\numberline {5.5.5}Monitoring via Tensorboard}{96}{subsection.5.5.5}
\contentsline {chapter}{\numberline {6}Results and Discussions}{97}{chapter.6}
\contentsline {section}{\numberline {6.1}Distributed Training}{97}{section.6.1}
\contentsline {section}{\numberline {6.2}Pure Reinforcement Learning Methods Results}{98}{section.6.2}
\contentsline {subsection}{\numberline {6.2.1}RNR}{99}{subsection.6.2.1}
\contentsline {subsection}{\numberline {6.2.2}RRR}{101}{subsection.6.2.2}
\contentsline {subsection}{\numberline {6.2.3}RNR+RRR}{101}{subsection.6.2.3}
\contentsline {subsection}{\numberline {6.2.4}RNR+RRR+RISD}{103}{subsection.6.2.4}
\contentsline {subsection}{\numberline {6.2.5}Reinforcement ``Supervised"}{105}{subsection.6.2.5}
\contentsline {subsection}{\numberline {6.2.6}Other ideas for pure RL techniques}{106}{subsection.6.2.6}
\contentsline {section}{\numberline {6.3}Hybrid Learning Models}{107}{section.6.3}
\contentsline {subsection}{\numberline {6.3.1}Supervised Learning Results}{107}{subsection.6.3.1}
\contentsline {subsubsection}{\numberline {6.3.1.1}Training Results}{107}{subsubsection.6.3.1.1}
\contentsline {subsubsection}{\numberline {6.3.1.2}The Learned Kick Motion}{108}{subsubsection.6.3.1.2}
\contentsline {subsubsection}{\numberline {6.3.1.3}The Learned Walk Motion}{110}{subsubsection.6.3.1.3}
\contentsline {subsubsection}{\numberline {6.3.1.4}Other motions}{111}{subsubsection.6.3.1.4}
\contentsline {subsection}{\numberline {6.3.2}HLM+RNR}{112}{subsection.6.3.2}
\contentsline {subsection}{\numberline {6.3.3}Final Model: HLM+RNR+RET}{113}{subsection.6.3.3}
\contentsline {subsection}{\numberline {6.3.4}Kick Behavior: Numerical Results}{115}{subsection.6.3.4}
\contentsline {chapter}{\numberline {7}Conclusions, Recommendations, and Future Works}{117}{chapter.7}
\contentsline {section}{\numberline {7.1}Conclusions}{117}{section.7.1}
\contentsline {section}{\numberline {7.2}Future Work}{118}{section.7.2}
\contentsline {chapter}{Bibliography}{119}{chapter.8}
\SetAppendixTocName {Appendix}
\contentsline {chapter}{\numberline {A}Hyperparameters}{126}{appendix.A}
