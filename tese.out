\BOOKMARK [0][]{cover.0}{Cover}{}% 1
\BOOKMARK [0][]{titulo.0}{Face Page}{}% 2
\BOOKMARK [0][]{cip.0}{Cataloging-in-Publication}{}% 3
\BOOKMARK [0][]{aprovacao.0}{Thesis Committee Composition:}{}% 4
\BOOKMARK [0][]{abstract.0}{Abstract}{}% 5
\BOOKMARK [0][]{listafiguras.0}{List of Figures}{}% 6
\BOOKMARK [0][]{listatabelas.0}{List of Tables}{}% 7
\BOOKMARK [0][]{contents.0}{Contents}{}% 8
\BOOKMARK [0][]{chapter.1}{1 Introduction}{}% 9
\BOOKMARK [1][]{section.1.1}{1.1 Motivation}{chapter.1}% 10
\BOOKMARK [1][]{section.1.2}{1.2 Contextualization}{chapter.1}% 11
\BOOKMARK [1][]{section.1.3}{1.3 Objective}{chapter.1}% 12
\BOOKMARK [1][]{section.1.4}{1.4 Scope}{chapter.1}% 13
\BOOKMARK [1][]{section.1.5}{1.5 Organization of this work}{chapter.1}% 14
\BOOKMARK [0][]{chapter.2}{2 Literature Review}{}% 15
\BOOKMARK [1][]{section.2.1}{2.1 The RoboCup Soccer3D Simulation League}{chapter.2}% 16
\BOOKMARK [2][]{subsection.2.1.1}{2.1.1 Domain Description}{section.2.1}% 17
\BOOKMARK [2][]{subsection.2.1.2}{2.1.2 \040Kick Motion }{section.2.1}% 18
\BOOKMARK [2][]{subsection.2.1.3}{2.1.3 Keyframe Movements}{section.2.1}% 19
\BOOKMARK [2][]{subsection.2.1.4}{2.1.4 Optimization Techniques}{section.2.1}% 20
\BOOKMARK [1][]{section.2.2}{2.2 Reinforcement Learning for Control}{chapter.2}% 21
\BOOKMARK [0][]{chapter.3}{3 Deep Learning}{}% 22
\BOOKMARK [1][]{section.3.1}{3.1 Neural Networks}{chapter.3}% 23
\BOOKMARK [0][]{chapter.4}{4 Methodology}{}% 24
\BOOKMARK [1][]{section.4.1}{4.1 Experimentation Setup}{chapter.4}% 25
\BOOKMARK [2][]{subsection.4.1.1}{4.1.1 Hybrid Learning Model \205 HLM}{section.4.1}% 26
\BOOKMARK [2][]{subsection.4.1.2}{4.1.2 Reinforcement with Naive Reward - RNR }{section.4.1}% 27
\BOOKMARK [2][]{subsection.4.1.3}{4.1.3 Reinforcement with Reference Reward - RRR }{section.4.1}% 28
\BOOKMARK [2][]{subsection.4.1.4}{4.1.4 Reinforcement with Initial State Distribution - RISD}{section.4.1}% 29
\BOOKMARK [2][]{subsection.4.1.5}{4.1.5 Reinforcement with Early Termination - RET}{section.4.1}% 30
\BOOKMARK [1][]{section.4.2}{4.2 Supervised Learning Setup}{chapter.4}% 31
\BOOKMARK [2][]{subsection.4.2.1}{4.2.1 The Dataset}{section.4.2}% 32
\BOOKMARK [2][]{subsection.4.2.2}{4.2.2 Neural Network Architecture and Hyperparameters}{section.4.2}% 33
\BOOKMARK [2][]{subsection.4.2.3}{4.2.3 The Training Procedure}{section.4.2}% 34
\BOOKMARK [2][]{subsection.4.2.4}{4.2.4 The Deployment in the Soccer 3D Environment}{section.4.2}% 35
\BOOKMARK [1][]{section.4.3}{4.3 Reinforcement Learning Setup}{chapter.4}% 36
\BOOKMARK [2][]{subsection.4.3.1}{4.3.1 Policy Representation}{section.4.3}% 37
\BOOKMARK [2][]{subsection.4.3.2}{4.3.2 Task Description}{section.4.3}% 38
\BOOKMARK [1][]{section.4.4}{4.4 Infrastructure}{chapter.4}% 39
\BOOKMARK [2][]{subsection.4.4.1}{4.4.1 Reinforcement Learning Server}{section.4.4}% 40
\BOOKMARK [2][]{subsection.4.4.2}{4.4.2 Neural Network Deployment}{section.4.4}% 41
\BOOKMARK [2][]{subsection.4.4.3}{4.4.3 Distributed Training}{section.4.4}% 42
\BOOKMARK [2][]{subsection.4.4.4}{4.4.4 Metrics}{section.4.4}% 43
\BOOKMARK [2][]{subsection.4.4.5}{4.4.5 Monitoring via Tensorboard}{section.4.4}% 44
\BOOKMARK [0][]{chapter.5}{5 Results' Analysis and Discussion}{}% 45
\BOOKMARK [1][]{section.5.1}{5.1 Training Results}{chapter.5}% 46
\BOOKMARK [1][]{section.5.2}{5.2 The Learned Kick Motion}{chapter.5}% 47
\BOOKMARK [1][]{section.5.3}{5.3 The Learned Walk Motion}{chapter.5}% 48
\BOOKMARK [1][]{section.5.4}{5.4 Other motions}{chapter.5}% 49
\BOOKMARK [0][]{chapter.6}{6 Conclusions, Recommendations, and Future Works}{}% 50
\BOOKMARK [1][]{section.6.1}{6.1 Preliminary Conclusions and Future Works}{chapter.6}% 51
\BOOKMARK [1][]{section.6.2}{6.2 The Activities Plan}{chapter.6}% 52
\BOOKMARK [0][]{chapter.7}{Bibliography}{}% 53
\BOOKMARK [0][]{bla.0}{Folha de Registro do Documento}{}% 54
