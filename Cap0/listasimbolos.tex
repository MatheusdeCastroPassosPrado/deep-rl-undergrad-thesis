\begin{longtable}{ll}
{\bf Sets} \\
$K$ & Keyframe set \\
$\mathbb{R}$ & The set of real numbers \\
$\mathbb{A}$ & A set \\
\\
{\bf Numbers and Arrays} \\
$\boldsymbol{x}$ & A vector $\boldsymbol{x}$ \\
$A$ & Matrix $A$ \\
$\boldsymbol{I}$ & Identity matrix \\
$\boldsymbol{X}$ & Tensor $\boldsymbol{X}$ \\
\\

{\bf Linear Algebra Operations} \\
$\boldsymbol{A}^{T}$ & Transpose of matrix $\boldsymbol{A}$ \\
$\lVert \boldsymbol{x} \rVert$ & $L^{2}$ norm of $\boldsymbol{x}$ \\
$\boldsymbol{a} \odot \boldsymbol{b}$ & Element-wise multiplication of vectors $a$ and $b$ \\

\\

{\bf Calculus} \\
$\frac{dy}{dx}$ & Derivative of $y$ w.r.t $x$ \\
$\frac{\partial{J}}{\partial{x}}$ & Partial derivative of $J$ w.r.t $x$ \\
$\nabla_{\boldsymbol{X}}z$ & Gradient of $z$ w.r.t $\boldsymbol{X}$ \\
$\lim_{k \rightarrow b} a$ & Limit of $a$ when $k$ tends to $b$ \\

\\

{\bf Probabily and Information Theory} \\
$a \sim P$ & Random variable $a$ has distribution $P$ \\
$\mathbb{E}_{\mathrm{\mathbf{x}}\sim P}f(x)$ & Expectation of $f(x)$ with respect to $P(x)$ \\
$\mathcal{N}(\boldsymbol{x}; \boldsymbol{\mu}, \boldsymbol{\Sigma})$ & Gaussian distribution over
$\boldsymbol{x}$ with mean $\boldsymbol{\mu}$ and covariance $\boldsymbol{\Sigma}$ \\
$\sigma^{2}(W)$ & Variance of $W$ \\
$\mathbb{P}(X = x| Y = y)$ & Probability of $X = x$ given that $Y = y$ \\
$KL[P,Q]imim$ & Kullback-Leibler divergence of $P$ and $Q$\\
$\hat{x}$ & Prediction of function $x$ \\
$A^{(i)}$ & Estimator $A$ of $i^{th}$ order \\


\\

{\bf Functions} \\
$\mathcal{C}$ & Class of a function \\
$f$ & A function $f$ \\
$f(\boldsymbol{x}; \boldsymbol{\theta})$ & A function of $\boldsymbol{x}$ parameterized by $\boldsymbol{\theta}$ \\
$g(f(\boldsymbol{x}))$ & Composition of functions $g$ and $f$ \\
$f : A \rightarrow B$ & Function $f$ defined from set $A$  to set $B$ \\
$tanh(z)$ & Hyperbolic tangent function applied to $z$ \\
$max(a,b)$ & Max function applied to element $a$ and $b$ \\
$min(a,b)$ & Mun function applied to element $a$ and $b$ \\

\\

{\bf Deep Learning} \\
$\sigma(\boldsymbol{x})$ &\ Sigmoid function applied do vector
 $\boldsymbol{x}$ \\
 $a^{[i]}_{j}$ & $j^{th}$ neuron from $i^{th}$ layer in a neural network \\
 $W^{[i]}$ & Neural network parameters from $i^{th}$ layer \\
 $b^{[i]}$ & Bias parameters from $i^{th}$ layer \\
 $\hat{y}$ & Prediction of a neural network \\
 $J(\boldsymbol{\theta})$ & Cost function of $\boldsymbol{\theta}$ \\
  $W^{*}$, $b^{*}$ & Parameter weights $W$ and $b$ that minimizes cost function \\
 $\mathcal{L}(\hat{y}, y)$ & Loss function between prediction $\hat{y}$ and ground-truth $y$ \\
 $\alpha$ & Learning rate hyperparameter \\
 $\beta$ & Decay rate hyperparameter \\
 $\epsilon$ & Fuzz factor \\
 $v_{dW}$, $v_{db}$ & First order momentum \\
 $u_{dW}$, $u_{db}$ & Second order momentum \\
 $L$ & Lipschitz constant \\
 
 \\
 
{\bf Reinforcement Learning} \\

$a$ & Action $a$ \\
$s$ & State $s$ \\
$R$ & Reward $R$ \\
$H$ & Horizon $H$ \\
$\gamma$ & Discount factor \\
$\pi$ & Policy $\pi$\\
$v_{\pi}$ & Value-function of policy $\pi$ \\
$\mathcal{P}$ & Transition matrix\\
$\mathcal{A}$ & Set of actions \\
$\mathcal{S}$ & Set of states \\
$\mathcal{R}$ & Reward function \\
$G$ & Return function \\
$q_{\pi}$ & Action-value function of policy $\pi$ \\
$A_{\pi}$ & Advantage function of policy $\pi$\\
$\delta^{t}$ & Temporal-Difference error \\
$\lambda$ & GAE factor \\





















\end{longtable}

