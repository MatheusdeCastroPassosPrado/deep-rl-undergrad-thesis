\begin{longtable}{ll}
ACER & Actor-Critic with Experience Replay \\
A2C & Advantage-Actor-Critic \\
API & Application Programming Interface \\
BGC & Batch Gradient Descent \\
CPU & Central Processor Unit \\
CMA-ES & Covariance Matrix Adaptation Evolution Strategy \\
CEM & Cross-Entropy Method \\
DDPG & Deep Deterministic Policy Gradient \\
DAG & Directed Acyclic Graph \\
DPPO & Distributed Proximal Policy Optimization \\
DP & Dynamic Programming \\
GAE & Generalized Advantage Estimation \\
GAIL & Generative Adversarial Imitation Learning \\
GUI & Graphical User Interface \\
GPU & Graphic Processor Unit \\
HC & Hill Climbing \\
HER & Hindsight Experience Replay \\
HLM & Hybrid Learning Model \\
HTTP & Hypertext Transfer Protocol \\
KL & Kullback-Leibler \\
MDP & Markov Decision Process \\
MP & Markov Process \\
MRP & Markov Reward Process \\
ML & Machine Learning \\
MuJoCo & Multi-Joint dynamics with Contact \\
ODE & Open Dynamics Engine \\
PPO & Proximal Policy Optimization \\
ReLU & Rectified Linear Unit \\
RL & Reinforcement Learning \\
RISD & Reinforcement with Initial State Distribution \\
RET & Reinforcement with Early Termination \\
RNR & Reinforcement with Naive Reward \\
RRR & Reinforcement with Reference Reward \\
RPC & Remote Procedure Call \\
SIMD & Single Instruction Multiple Data \\
SGD & Stochastic Gradient Descent \\
TPU & Tensor Processor Unit \\
TCP & Transmission Control Protocol \\
TRPO & Trusted Region Policy Optimization \\
ZMP & Zero Moment Point \\

\end{longtable}

